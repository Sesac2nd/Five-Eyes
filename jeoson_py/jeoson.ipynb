{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efd821a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'keywords': ['ì¡°ì„  ì¤‘ê¸°', 'ì–‘ë°˜', 'ì‚¶'], 'ai_eval': 'ì „ë¬¸ê°€ í‰ê°€:\\n\\n1. **í‚¤ì›Œë“œ ë°˜ì˜ ì—¬ë¶€**  \\nì§ˆë¬¸ì—ì„œ ì œì‹œëœ í‚¤ì›Œë“œì¸ â€˜ì¡°ì„  ì¤‘ê¸°â€™, â€˜ì–‘ë°˜â€™, â€˜ì‚¶â€™ì´ ëª¨ë‘ ì¶©ì‹¤í•˜ê²Œ ë°˜ì˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë‹µë³€ì€ ì¡°ì„  ì¤‘ê¸°ë¼ëŠ” ì‹œê¸°ì  ë²”ìœ„ì™€ ì–‘ë°˜ ê³„ì¸µì— ì´ˆì ì„ ë‘ê³ , ê²½ì œ ìƒí™œ, ì‚¬íšŒì  íŠ¹ê¶Œ, ì¼ìƒ/êµìœ¡, ì •ì¹˜ ì°¸ì—¬ ë“± ì‚¶ì˜ ë‹¤ì–‘í•œ ì¸¡ë©´ì„ ì‹¤ë¡ì˜ êµ¬ì²´ì  ê¸°ì‚¬ì™€ ì—°ê²°í•´ ì„¤ëª…í•˜ê³  ìˆìŠµë‹ˆë‹¤.\\n\\n2. **ì‚¬ë£Œ(ì‹¤ë¡) ì¸ìš© ë° ì¶œì²˜ í‘œê¸° ì ì ˆì„±**  \\nê° í•­ëª©ë§ˆë‹¤ ì‹¤ë¡ì˜ íŠ¹ì • ê¸°ì‚¬(ê¶Œ, ì—°ë„, ë‚ ì§œ)ë¥¼ ëª…í™•íˆ í‘œê¸°í•˜ê³  í•´ë‹¹ ê¸°ì‚¬ì—ì„œ ë°œì·Œí•œ ë‚´ìš©(ì§ì ‘ ì¸ìš© ë˜ëŠ” ìš”ì•½)ì„ ì œì‹œí•˜ì˜€ìœ¼ë¯€ë¡œ, ì‚¬ë£Œì— ê·¼ê±°í•œ ì„¤ëª…ì´ë¼ëŠ” ì ì´ ëª…í™•í•˜ê²Œ ë“œëŸ¬ë‚©ë‹ˆë‹¤.  \\nì˜ˆ)', 'answer': 'ì¡°ì„  ì¤‘ê¸°ì˜ ì–‘ë°˜ë“¤ì˜ ì‚¶ì— ëŒ€í•´ ì¡°ì„ ì™•ì¡°ì‹¤ë¡ì— ê·¼ê±°í•˜ì—¬ ë§ì”€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\\n\\n1. ê²½ì œ ìƒí™œ  \\nì¡°ì„  ì¤‘ê¸° ì–‘ë°˜ë“¤ì€ í† ì§€ë¥¼ ì†Œìœ í•˜ì—¬ ë†ë¯¼ì—ê²Œ ì†Œì‘ì„ ì£¼ê³  ê·¸ ìˆ˜í™•ì˜ ì¼ë¶€ë¥¼ ìˆ˜ì·¨í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ì—ˆìŠµë‹ˆë‹¤. ã€Šì¤‘ì¢…ì‹¤ë¡ã€‹ 26ê¶Œ, ì¤‘ì¢… 12ë…„ 9ì›” 3ì¼ì ê¸°ì‚¬ì—ëŠ” \"ì–‘ë°˜ë“¤ì´ ì „ë‹µì„ ë§ì´ ì†Œìœ í•˜ì—¬ ë°±ì„±ë“¤ì˜ í˜•í¸ì´ ê³¤ë€í•˜ë‹¤\"ë¼ëŠ” ì–¸ê¸‰ì´ ìˆì–´, ì–‘ë°˜ì˜ í† ì§€ ì†Œìœ ì™€ ë†ë¯¼ê³¼ì˜ ê´€ê³„ê°€ ë“œëŸ¬ë‚©ë‹ˆë‹¤.\\n\\n2. ì‚¬íšŒì  íŠ¹ê¶Œ  \\nì–‘ë°˜ì€ ê³¼ê±°ì œ(ç§‘æ“§åˆ¶)ë¥¼ í†µí•´ ê´€ì§ì— ë‚˜ì•„ê°ˆ ìˆ˜ ìˆëŠ” ì‹ ë¶„ì´ì—ˆìœ¼ë©°, ë²•ì ìœ¼ë¡œë„ ì—¬ëŸ¬ íŠ¹ê¶Œì„ ëˆ„ë ¸ìŠµë‹ˆë‹¤. ã€Šì„ ì¡°ì‹¤ë¡ã€‹ 69ê¶Œ, ì„ ì¡° 50ë…„ 1ì›” 10ì¼ì ê¸°ì‚¬ì—ì„œëŠ” \"ì–‘ë°˜ ìì œë§Œ ê³¼ê±°ì— ì‘ì‹œí•  ìˆ˜ ìˆìœ¼ë‹ˆ, ì‹ ë¶„ì— ë”°ë¼ ê´€ì§ ì§„ì¶œì´ ì œí•œëœë‹¤\"ê³  ê¸°ë¡ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\\n\\n3. ì¼ìƒ ë° êµìœ¡  \\nì–‘ë°˜ ê°€ë¬¸ì—ì„œëŠ” ìì œë“¤ì—ê²Œ ìœ í•™(å„’å­¸)ì„ ê°€ë¥´ì¹˜ê³ , íš¨(å­)ì™€ ì˜ˆ(ç¦®)ë¥¼ ì¤‘ì‹œí•˜ì˜€ìŠµë‹ˆë‹¤. ã€Šê´‘í•´êµ°ì¼ê¸°ã€‹ 93ê¶Œ, ê´‘í•´êµ° 7ë…„ 12ì›” 19ì¼ì ê¸°ì‚¬ì—ëŠ” \"ì–‘ë°˜ ì§‘ì•ˆì—ì„œëŠ” ì•„ì¹¨ì €ë…ìœ¼ë¡œ ê¸€ ì½ëŠ” ì†Œë¦¬ê°€ ëŠì´ì§€ ì•ŠëŠ”ë‹¤\"ê³  ì–¸ê¸‰ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\\n\\n4. ì •ì¹˜ ì°¸ì—¬ì™€ í–¥ì´Œ ìš´ì˜  \\nì–‘ë°˜ë“¤ì€ ì§€ë°©ì˜ í–¥ì•½(é„•ç´„) ë“± ìì¹˜ ì¡°ì§ì„ ìš´ì˜í•˜ë©° í–¥ì´Œ ì‚¬íšŒë¥¼ ì£¼ë„í•˜ì˜€ìŠµë‹ˆë‹¤. ã€Šì¤‘ì¢…ì‹¤ë¡ã€‹ 75ê¶Œ, ì¤‘ì¢… 28ë…„ 6ì›” 13ì¼ì ê¸°ì‚¬ì—ëŠ” \"í–¥ì•½ì„ ë‘ì–´ ì–‘ë°˜ë“¤ì´ ë°±ì„±ì„ êµí™”í•˜ê³  ë‹¤ìŠ¤ë¦°ë‹¤\"ê³  ë‚˜ì™€ ìˆìŠµë‹ˆë‹¤.\\n\\nì´ìƒì€ ì¡°ì„ ì™•ì¡°ì‹¤ë¡ì— ëª…ì‹œëœ ì–‘ë°˜ë“¤ì˜ ì‚¶ì— ê´€í•œ ì£¼ìš” ë‚´ìš©ì…ë‹ˆë‹¤.'}\n"
     ]
    }
   ],
   "source": [
    "#ì¡°ì„ ì™•ì¡°ì‹¤ë¡ rag\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT = \"https://5team-history-azureai-services.openai.azure.com/\"\n",
    "AZURE_OPENAI_DEPLOYMENT = \"jeoson-rag\"\n",
    "AZURE_OPENAI_API_VERSION = \"2025-01-01-preview\"\n",
    "AZURE_OPENAI_API_KEY = \"api.key\"\n",
    "\n",
    "# í´ë¼ì´ì–¸íŠ¸ ìƒì„±\n",
    "client = AzureOpenAI(\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=AZURE_OPENAI_API_VERSION,\n",
    ")\n",
    "\n",
    "system_message = (\n",
    "    \"ë„ˆëŠ” RAG ì‹œìŠ¤í…œê³¼ ì—°ë™ëœ ì¡°ì„ ì‹œëŒ€ í•œêµ­ì‚¬ ì „ë¬¸ê°€ì•¼. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ë²¡í„° ê²€ìƒ‰ëœ ì‚¬ë£Œ ì²­í¬ë¥¼ ë°”íƒ•ìœ¼ë¡œë§Œ ë‹µë³€í•˜ê³ , ë°˜ë“œì‹œ í•´ë‹¹ ì²­í¬ì—ì„œ ë°œì·Œí•œ ë¬¸ì¥ì„ ëª…ì‹œí•˜ê±°ë‚˜ ì¶œì²˜ë¥¼ í‘œê¸°í•´ì•¼ í•´. ì‚¬ë£Œì— ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³  'í•´ë‹¹ ì •ë³´ëŠ” ì‹¤ë¡ì—ì„œ í™•ì¸ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤'ë¼ê³  ë§í•´ì¤˜\"\n",
    ")\n",
    "\n",
    "def extract_keywords(query):\n",
    "    prompt = f\"ë‹¤ìŒ ë¬¸ì¥ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ 3~5ê°œë§Œ 'í‚¤ì›Œë“œ1, í‚¤ì›Œë“œ2, ...' í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•´ì¤˜:\\n{query}\\n>\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=AZURE_OPENAI_DEPLOYMENT,\n",
    "        max_tokens=100,\n",
    "        temperature=0.7,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "    keywords = response.choices[0].message.content.strip()\n",
    "    return [kw.strip() for kw in keywords.split(\",\")]\n",
    "\n",
    "def evaluate_answer(keywords, answer):\n",
    "    eval_prompt = (\n",
    "        f\"ë‹¤ìŒ í‚¤ì›Œë“œ: {', '.join(keywords)}\\nì•„ë˜ ë‹µë³€ì´ í‚¤ì›Œë“œë¥¼ ì˜ ë°˜ì˜í•˜ê³  ìˆëŠ”ì§€ \"\n",
    "        f\"ì¡°ì„ ì‹œëŒ€ í•œêµ­ì‚¬ ì „ë¬¸ê°€ AI ì‹œê°ìœ¼ë¡œ í‰ê°€í•´ì¤˜:\\n{answer}\"\n",
    "    )\n",
    "    response = client.chat.completions.create(\n",
    "        model=AZURE_OPENAI_DEPLOYMENT,\n",
    "        max_tokens=200,\n",
    "        temperature=0.7,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": eval_prompt},\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def get_answer(query):\n",
    "    rag_prompt = (\n",
    "        f\"{query}\\n(ë‹¤ìŒ ë‚´ìš©ì€ ì¡°ì„ ì™•ì¡°ì‹¤ë¡ì„ ì°¸ì¡°í•˜ì—¬ ê³ ì¦í•œ ë‚´ìš©ìœ¼ë¡œ ì‘ì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.)\"\n",
    "    )\n",
    "    response = client.chat.completions.create(\n",
    "        model=AZURE_OPENAI_DEPLOYMENT,\n",
    "        max_tokens=1024,\n",
    "        temperature=0.7,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": rag_prompt},\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"ì¡°ì„  ì¤‘ê¸°ì˜ ì–‘ë°˜ë“¤ì˜ ì‚¶\"\n",
    "\n",
    "    # 1. í‚¤ì›Œë“œ ì¶”ì¶œ\n",
    "    keywords = extract_keywords(query)\n",
    "\n",
    "    # 2. ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±\n",
    "    answer = get_answer(query)\n",
    "\n",
    "    # 3. AIê°€ ë‹µë³€ê³¼ í‚¤ì›Œë“œ ë§¤ì¹­ í‰ê°€\n",
    "    ai_eval = evaluate_answer(keywords, answer)\n",
    "\n",
    "    # 4. JSON ê²°ê³¼ êµ¬ì„± ë° ì¶œë ¥\n",
    "    result_json = {\n",
    "        \"keywords\": keywords,\n",
    "        \"ai_eval\": ai_eval,\n",
    "        \"answer\": answer,\n",
    "    }\n",
    "    print(result_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429acbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ì¡°ì„ ì™•ì¡°ì‹¤ë¡ ì‹œë†‰ì‹œìŠ¤rag\n",
    "import os\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from openai import AzureOpenAI\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "AZURE_SEARCH_ENDPOINT = \"https://5team-ai-search.search.windows.net\"\n",
    "AZURE_SEARCH_INDEX_NAME = \"rag-ksat\"\n",
    "AZURE_SEARCH_API_KEY = \"apikey\"\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT = \"https://5team-history-azureai-services.openai.azure.com/\"\n",
    "AZURE_OPENAI_DEPLOYMENT = \"jeoson-rag\"\n",
    "AZURE_OPENAI_API_VERSION = \"2025-01-01-preview\"\n",
    "AZURE_OPENAI_API_KEY = \"apikey\"\n",
    "\n",
    "# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\n",
    "search_client = SearchClient(\n",
    "    endpoint=AZURE_SEARCH_ENDPOINT,\n",
    "    index_name=AZURE_SEARCH_INDEX_NAME,\n",
    "    credential=AzureKeyCredential(AZURE_SEARCH_API_KEY)\n",
    ")\n",
    "\n",
    "openai_client = AzureOpenAI(\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=AZURE_OPENAI_API_VERSION,\n",
    ")\n",
    "\n",
    "def print_header(title, width=80):\n",
    "    \"\"\"í—¤ë” ì¶œë ¥\"\"\"\n",
    "    print(\"\\n\" + \"=\"*width)\n",
    "    print(f\"ğŸ¯ {title}\")\n",
    "    print(\"=\"*width)\n",
    "\n",
    "def print_section(title, width=60):\n",
    "    \"\"\"ì„¹ì…˜ ì¶œë ¥\"\"\"\n",
    "    print(f\"\\n{'='*10} {title} {'='*10}\")\n",
    "\n",
    "def print_subsection(title):\n",
    "    \"\"\"í•˜ìœ„ ì„¹ì…˜ ì¶œë ¥\"\"\"\n",
    "    print(f\"\\nğŸ“Œ {title}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "def extract_keywords_from_query(query, top_k=10, max_final_docs=10):\n",
    "    \"\"\"\n",
    "    AIë¥¼ ì´ìš©í•´ ì¿¼ë¦¬ì—ì„œ ê²€ìƒ‰ í‚¤ì›Œë“œ ìë™ ì¶”ì¶œ\n",
    "    \n",
    "    Args:\n",
    "        query (str): ì‚¬ìš©ì ì…ë ¥ ì¿¼ë¦¬\n",
    "        top_k (int): í‚¤ì›Œë“œë‹¹ ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜\n",
    "        max_final_docs (int): ìµœì¢… ì„ ë³„í•  ë¬¸ì„œ ìˆ˜\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸, í‚¤ì›Œë“œ ì¶”ì¶œ ìƒì„¸ ì •ë³´)\n",
    "    \"\"\"\n",
    "    print_subsection(\"AI ê¸°ë°˜ í‚¤ì›Œë“œ ìë™ ì¶”ì¶œ\")\n",
    "    print(f\"ğŸ” ì…ë ¥ ì¿¼ë¦¬: {query}\")\n",
    "    \n",
    "    keyword_extraction_prompt = f\"\"\"\n",
    "ì‚¬ìš©ì ì¿¼ë¦¬: \"{query}\"\n",
    "\n",
    "ìœ„ ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ì—¬ Azure Searchì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ê¸° ìœ„í•œ ìµœì ì˜ ê²€ìƒ‰ í‚¤ì›Œë“œë“¤ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.\n",
    "\n",
    "ì¶”ì¶œ ê¸°ì¤€:\n",
    "1. ì¿¼ë¦¬ì˜ í•µì‹¬ ì£¼ì œì™€ ê´€ë ¨ëœ í‚¤ì›Œë“œ\n",
    "2. ì—­ì‚¬ì  ì¸ë¬¼ëª…, ì‚¬ê±´ëª…, ì œë„ëª…\n",
    "3. ì‹œëŒ€ì  ë°°ê²½ í‚¤ì›Œë“œ  \n",
    "4. ê´€ë ¨ ê°œë…ì–´ ë° ì „ë¬¸ìš©ì–´\n",
    "5. ìœ ì‚¬í•œ ì˜ë¯¸ì˜ ë™ì˜ì–´ë“¤\n",
    "6. ê²€ìƒ‰ì— ìœ ìš©í•œ ë‹¨ì¼ì–´ ìš°ì„  (2-3ê¸€ì ë‹¨ì–´ë“¤)\n",
    "\n",
    "ì‘ë‹µ í˜•ì‹:\n",
    "í•µì‹¬í‚¤ì›Œë“œ: [ê°€ì¥ ì¤‘ìš”í•œ í‚¤ì›Œë“œ 5ê°œë¥¼ ì½¤ë§ˆë¡œ êµ¬ë¶„]\n",
    "í™•ì¥í‚¤ì›Œë“œ: [ê´€ë ¨ëœ ì¶”ê°€ í‚¤ì›Œë“œ 10ê°œë¥¼ ì½¤ë§ˆë¡œ êµ¬ë¶„] \n",
    "ë™ì˜ì–´í‚¤ì›Œë“œ: [ë™ì˜ì–´ë‚˜ ìœ ì‚¬ì–´ 5ê°œë¥¼ ì½¤ë§ˆë¡œ êµ¬ë¶„]\n",
    "ì¸ë¬¼í‚¤ì›Œë“œ: [ê´€ë ¨ ì¸ë¬¼ëª… 5ê°œë¥¼ ì½¤ë§ˆë¡œ êµ¬ë¶„ - ìˆë‹¤ë©´]\n",
    "ì‹œëŒ€í‚¤ì›Œë“œ: [ì‹œëŒ€/ì™•ì¡° ê´€ë ¨ 3ê°œë¥¼ ì½¤ë§ˆë¡œ êµ¬ë¶„ - ìˆë‹¤ë©´]\n",
    "\n",
    "ì¶”ì¶œ ì´ìœ : [í‚¤ì›Œë“œ ì„ íƒ ì´ìœ ë¥¼ 2-3ì¤„ë¡œ ì„¤ëª…]\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"ğŸ¤– AI í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...\")\n",
    "        \n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=AZURE_OPENAI_DEPLOYMENT,\n",
    "            max_tokens=800,\n",
    "            temperature=0.3,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\", \n",
    "                    \"content\": \"í•œêµ­ì‚¬ ì „ë¬¸ê°€ì´ì ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ì¶œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ì ì¿¼ë¦¬ì—ì„œ ë¬¸ì„œ ê²€ìƒ‰ì— ê°€ì¥ íš¨ê³¼ì ì¸ í‚¤ì›Œë“œë“¤ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": keyword_extraction_prompt\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        extraction_result = response.choices[0].message.content.strip()\n",
    "        print(\"âœ… AI í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ\")\n",
    "        \n",
    "        # í‚¤ì›Œë“œ íŒŒì‹±\n",
    "        keywords = []\n",
    "        extraction_info = {\n",
    "            'core': [],\n",
    "            'extended': [], \n",
    "            'synonyms': [],\n",
    "            'persons': [],\n",
    "            'periods': [],\n",
    "            'reasoning': ''\n",
    "        }\n",
    "        \n",
    "        lines = extraction_result.split('\\n')\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line.startswith('í•µì‹¬í‚¤ì›Œë“œ:'):\n",
    "                core_kw = line.replace('í•µì‹¬í‚¤ì›Œë“œ:', '').strip()\n",
    "                extraction_info['core'] = [kw.strip() for kw in core_kw.split(',') if kw.strip()]\n",
    "                keywords.extend(extraction_info['core'])\n",
    "                \n",
    "            elif line.startswith('í™•ì¥í‚¤ì›Œë“œ:'):\n",
    "                ext_kw = line.replace('í™•ì¥í‚¤ì›Œë“œ:', '').strip()\n",
    "                extraction_info['extended'] = [kw.strip() for kw in ext_kw.split(',') if kw.strip()]\n",
    "                keywords.extend(extraction_info['extended'])\n",
    "                \n",
    "            elif line.startswith('ë™ì˜ì–´í‚¤ì›Œë“œ:'):\n",
    "                syn_kw = line.replace('ë™ì˜ì–´í‚¤ì›Œë“œ:', '').strip()\n",
    "                extraction_info['synonyms'] = [kw.strip() for kw in syn_kw.split(',') if kw.strip()]\n",
    "                keywords.extend(extraction_info['synonyms'])\n",
    "                \n",
    "            elif line.startswith('ì¸ë¬¼í‚¤ì›Œë“œ:'):\n",
    "                person_kw = line.replace('ì¸ë¬¼í‚¤ì›Œë“œ:', '').strip()\n",
    "                extraction_info['persons'] = [kw.strip() for kw in person_kw.split(',') if kw.strip()]\n",
    "                keywords.extend(extraction_info['persons'])\n",
    "                \n",
    "            elif line.startswith('ì‹œëŒ€í‚¤ì›Œë“œ:'):\n",
    "                period_kw = line.replace('ì‹œëŒ€í‚¤ì›Œë“œ:', '').strip()\n",
    "                extraction_info['periods'] = [kw.strip() for kw in period_kw.split(',') if kw.strip()]\n",
    "                keywords.extend(extraction_info['periods'])\n",
    "                \n",
    "            elif line.startswith('ì¶”ì¶œ ì´ìœ :'):\n",
    "                extraction_info['reasoning'] = line.replace('ì¶”ì¶œ ì´ìœ :', '').strip()\n",
    "        \n",
    "        # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬\n",
    "        unique_keywords = []\n",
    "        seen = set()\n",
    "        for kw in keywords:\n",
    "            if kw and kw not in seen and len(kw.strip()) >= 2:\n",
    "                unique_keywords.append(kw.strip())\n",
    "                seen.add(kw)\n",
    "        \n",
    "        print(f\"ğŸ“Š ì¶”ì¶œëœ í‚¤ì›Œë“œ í†µê³„:\")\n",
    "        print(f\"   - í•µì‹¬í‚¤ì›Œë“œ: {len(extraction_info['core'])}ê°œ\")\n",
    "        print(f\"   - í™•ì¥í‚¤ì›Œë“œ: {len(extraction_info['extended'])}ê°œ\") \n",
    "        print(f\"   - ë™ì˜ì–´í‚¤ì›Œë“œ: {len(extraction_info['synonyms'])}ê°œ\")\n",
    "        print(f\"   - ì¸ë¬¼í‚¤ì›Œë“œ: {len(extraction_info['persons'])}ê°œ\")\n",
    "        print(f\"   - ì‹œëŒ€í‚¤ì›Œë“œ: {len(extraction_info['periods'])}ê°œ\")\n",
    "        print(f\"   - ì´ ìœ íš¨ í‚¤ì›Œë“œ: {len(unique_keywords)}ê°œ\")\n",
    "        \n",
    "        print(f\"\\nğŸ”‘ ìµœì¢… í‚¤ì›Œë“œ ëª©ë¡:\")\n",
    "        for i, kw in enumerate(unique_keywords[:20], 1):  # ìƒìœ„ 20ê°œë§Œ ì¶œë ¥\n",
    "            print(f\"   {i:2d}. {kw}\")\n",
    "        \n",
    "        if len(unique_keywords) > 20:\n",
    "            print(f\"   ... ì™¸ {len(unique_keywords) - 20}ê°œ\")\n",
    "            \n",
    "        print(f\"\\nğŸ’¡ ì¶”ì¶œ ì´ìœ : {extraction_info['reasoning']}\")\n",
    "        \n",
    "        return unique_keywords, extraction_info\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}\")\n",
    "        # ì‹¤íŒ¨ ì‹œ ì¿¼ë¦¬ì—ì„œ ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ\n",
    "        fallback_keywords = []\n",
    "        # í•œê¸€ 2ê¸€ì ì´ìƒ ë‹¨ì–´ ì¶”ì¶œ\n",
    "        korean_words = re.findall(r'[ê°€-í£]{2,}', query)\n",
    "        fallback_keywords.extend(korean_words[:10])\n",
    "        \n",
    "        print(f\"ğŸ”„ ëŒ€ì•ˆ í‚¤ì›Œë“œ ì‚¬ìš©: {fallback_keywords}\")\n",
    "        return fallback_keywords, {'reasoning': 'ìë™ ì¶”ì¶œ ì‹¤íŒ¨ë¡œ ì¿¼ë¦¬ ë‹¨ì–´ ì‚¬ìš©'}\n",
    "\n",
    "def search_documents_with_keywords(keywords, top_k=10, max_final_docs=10):\n",
    "    \"\"\"\n",
    "    ì¶”ì¶œëœ í‚¤ì›Œë“œë¡œ ë¬¸ì„œ ê²€ìƒ‰\n",
    "    \n",
    "    Args:\n",
    "        keywords (list): ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸\n",
    "        top_k (int): í‚¤ì›Œë“œë‹¹ ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜  \n",
    "        max_final_docs (int): ìµœì¢… ì„ ë³„í•  ë¬¸ì„œ ìˆ˜\n",
    "        \n",
    "    Returns:\n",
    "        list: ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œë“¤\n",
    "    \"\"\"\n",
    "    print_subsection(f\"í‚¤ì›Œë“œ ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰\")\n",
    "    print(f\"ğŸ” ê²€ìƒ‰ í‚¤ì›Œë“œ: {len(keywords)}ê°œ\")\n",
    "    print(f\"ğŸ“Š ê²€ìƒ‰ ì„¤ì •: í‚¤ì›Œë“œë‹¹ {top_k}ê°œ, ìµœì¢… {max_final_docs}ê°œ\")\n",
    "    \n",
    "    all_docs = []\n",
    "    search_stats = {\"total_searched\": 0, \"found_docs\": 0, \"total_results\": 0}\n",
    "    \n",
    "    print(f\"\\n{'í‚¤ì›Œë“œ':<15} | {'ê²€ìƒ‰ê²°ê³¼':<8} | {'ê´€ë ¨ë¬¸ì„œ':<8} | {'ìµœê³ ì ìˆ˜':<10} | {'ìƒíƒœ'}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        try:\n",
    "            results = search_client.search(\n",
    "                search_text=keyword,\n",
    "                top=top_k,\n",
    "                search_mode=\"any\"\n",
    "            )\n",
    "            \n",
    "            search_stats[\"total_searched\"] += 1\n",
    "            found_count = 0\n",
    "            max_score = 0\n",
    "            \n",
    "            for result in results:\n",
    "                chunk = result.get('chunk', '')\n",
    "                score = result.get('@search.score', 0)\n",
    "                search_stats[\"total_results\"] += 1\n",
    "                max_score = max(max_score, score)\n",
    "                \n",
    "                if chunk and len(chunk.strip()) > 50:  # ì˜ë¯¸ìˆëŠ” ë‚´ìš©ë§Œ\n",
    "                    clean_preview = ' '.join(chunk.replace('\\n', ' ').split())\n",
    "                    preview_text = clean_preview[:100] + \"...\" if len(clean_preview) > 100 else clean_preview\n",
    "                    \n",
    "                    all_docs.append({\n",
    "                        'keyword': keyword,\n",
    "                        'chunk': chunk,\n",
    "                        'score': score,\n",
    "                        'preview': preview_text\n",
    "                    })\n",
    "                    found_count += 1\n",
    "            \n",
    "            if found_count > 0:\n",
    "                search_stats[\"found_docs\"] += 1\n",
    "                status = \"âœ… ë°œê²¬\"\n",
    "            else:\n",
    "                status = \"âŒ ì—†ìŒ\"\n",
    "                \n",
    "            print(f\"{keyword:<15} | {search_stats['total_results']:>8} | {found_count:>8} | {max_score:>10.2f} | {status}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"{keyword:<15} | {'ì˜¤ë¥˜':>8} | {'0':>8} | {'0.00':>10} | âš ï¸ ì‹¤íŒ¨\")\n",
    "    \n",
    "    print_subsection(\"ê²€ìƒ‰ í†µê³„\")\n",
    "    print(f\"ğŸ“Š ì´ ê²€ìƒ‰ í‚¤ì›Œë“œ: {search_stats['total_searched']}ê°œ\")\n",
    "    print(f\"ğŸ“Š ì´ ê²€ìƒ‰ëœ ë¬¸ì„œ: {search_stats['total_results']}ê°œ\")\n",
    "    print(f\"ğŸ“Š ìœ íš¨ ë¬¸ì„œ: {len(all_docs)}ê°œ\")\n",
    "    print(f\"ğŸ“Š í‚¤ì›Œë“œ ì„±ê³µë¥ : {(search_stats['found_docs']/max(search_stats['total_searched'], 1)*100):.1f}%\")\n",
    "    \n",
    "    # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬ ë° ì¤‘ë³µ ì œê±°\n",
    "    all_docs.sort(key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    unique_docs = []\n",
    "    seen_previews = set()\n",
    "    \n",
    "    for doc in all_docs:\n",
    "        # ë¬¸ì„œ ë‚´ìš©ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬ (ì•ì˜ 200ì)\n",
    "        content_key = ' '.join(doc['chunk'][:200].replace('\\n', ' ').split())\n",
    "        if content_key not in seen_previews:\n",
    "            unique_docs.append(doc)\n",
    "            seen_previews.add(content_key)\n",
    "            if len(unique_docs) >= max_final_docs:\n",
    "                break\n",
    "    \n",
    "    print_subsection(f\"ìµœì¢… ì„ ë³„ ë¬¸ì„œ ({len(unique_docs)}ê°œ)\")\n",
    "    print(f\"{'ìˆœìœ„':<4} | {'í‚¤ì›Œë“œ':<12} | {'ê²€ìƒ‰ì ìˆ˜':<10} | {'ë¯¸ë¦¬ë³´ê¸°'}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, doc in enumerate(unique_docs, 1):\n",
    "        clean_preview = ' '.join(doc['preview'].replace('\\n', ' ').split())[:50]\n",
    "        print(f\"{i:<4} | {doc['keyword']:<12} | {doc['score']:>10.2f} | {clean_preview}...\")\n",
    "    \n",
    "    return unique_docs\n",
    "\n",
    "def find_documents_from_query(query, top_k=10, max_final_docs=10):\n",
    "    \"\"\"\n",
    "    ì¿¼ë¦¬ì—ì„œ í‚¤ì›Œë“œë¥¼ ìë™ ì¶”ì¶œí•˜ê³  ë¬¸ì„œ ê²€ìƒ‰í•˜ëŠ” í†µí•© í•¨ìˆ˜\n",
    "    \n",
    "    Args:\n",
    "        query (str): ì‚¬ìš©ì ì…ë ¥ ì¿¼ë¦¬\n",
    "        top_k (int): í‚¤ì›Œë“œë‹¹ ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜\n",
    "        max_final_docs (int): ìµœì¢… ì„ ë³„í•  ë¬¸ì„œ ìˆ˜\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (ê²€ìƒ‰ëœ ë¬¸ì„œë“¤, í‚¤ì›Œë“œ ì •ë³´)\n",
    "    \"\"\"\n",
    "    print_header(\"ì¿¼ë¦¬ ê¸°ë°˜ ìë™ ë¬¸ì„œ ê²€ìƒ‰ ì‹œìŠ¤í…œ\")\n",
    "    print(f\"ğŸ“ ì…ë ¥ ì¿¼ë¦¬: {query}\")\n",
    "    print(f\"â° ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # 1ë‹¨ê³„: AI í‚¤ì›Œë“œ ì¶”ì¶œ\n",
    "    print_section(\"1ï¸âƒ£ AI í‚¤ì›Œë“œ ì¶”ì¶œ\")\n",
    "    keywords, keyword_info = extract_keywords_from_query(query, top_k, max_final_docs)\n",
    "    \n",
    "    if not keywords:\n",
    "        print(\"âŒ í‚¤ì›Œë“œ ì¶”ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\")\n",
    "        return [], {}\n",
    "    \n",
    "    # 2ë‹¨ê³„: í‚¤ì›Œë“œë¡œ ë¬¸ì„œ ê²€ìƒ‰\n",
    "    print_section(\"2ï¸âƒ£ í‚¤ì›Œë“œ ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰\")\n",
    "    documents = search_documents_with_keywords(keywords, top_k, max_final_docs)\n",
    "    \n",
    "    return documents, keyword_info\n",
    "\n",
    "def create_context_from_best_docs(docs):\n",
    "    \"\"\"ë¬¸ì„œì—ì„œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±\"\"\"\n",
    "    if not docs:\n",
    "        return \"ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "    \n",
    "    print_subsection(\"ì»¨í…ìŠ¤íŠ¸ ìƒì„±\")\n",
    "    \n",
    "    context_parts = []\n",
    "    total_length = 0\n",
    "    \n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        doc_content = f\"=== ì°¸ê³ ë¬¸ì„œ {i} (í‚¤ì›Œë“œ: {doc['keyword']}, ì ìˆ˜: {doc['score']:.2f}) ===\\n{doc['chunk']}\"\n",
    "        context_parts.append(doc_content)\n",
    "        total_length += len(doc['chunk'])\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    print(f\"ğŸ“„ ì´ ë¬¸ì„œ ìˆ˜: {len(docs)}ê°œ\")\n",
    "    print(f\"ğŸ“„ ì´ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(context):,}ì\")\n",
    "    print(f\"ğŸ“„ í‰ê·  ë¬¸ì„œ ê¸¸ì´: {total_length//len(docs):,}ì\")\n",
    "    \n",
    "    return context\n",
    "\n",
    "def smart_synopsis_generation(query, context):\n",
    "    \"\"\"ìŠ¤ë§ˆíŠ¸ ì‹œë†‰ì‹œìŠ¤ ìƒì„±\"\"\"\n",
    "    print_subsection(\"AI ê¸°ë°˜ ë‚´ìš© ë¶„ì„\")\n",
    "    \n",
    "    # 1ë‹¨ê³„: ê´€ë ¨ì„± í‰ê°€\n",
    "    print(\"ğŸ” 1ë‹¨ê³„: ìë£Œ ê´€ë ¨ì„± í‰ê°€ ì¤‘...\")\n",
    "    relevance_check_prompt = f\"\"\"\n",
    "ì§ˆë¬¸: {query}\n",
    "ì œê³µëœ ìë£Œ: {context[:1500]}...\n",
    "\n",
    "ì´ ìë£Œê°€ ì§ˆë¬¸ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ì´ ìˆëŠ”ì§€ 1-10ì ìœ¼ë¡œ í‰ê°€í•˜ê³ , \n",
    "ì£¼ìš” ê´€ë ¨ ë‚´ìš©ì„ 3-5ê°œ ë¶ˆë¦¿í¬ì¸íŠ¸ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.\n",
    "\n",
    "í‰ê°€ í˜•ì‹:\n",
    "ì ìˆ˜: X/10\n",
    "ì£¼ìš” ë‚´ìš©:\n",
    "- ë‚´ìš©1\n",
    "- ë‚´ìš©2\n",
    "- ë‚´ìš©3\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        relevance_response = openai_client.chat.completions.create(\n",
    "            model=AZURE_OPENAI_DEPLOYMENT,\n",
    "            max_tokens=400,\n",
    "            temperature=0.1,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"ìë£Œ ê´€ë ¨ì„± í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\"},\n",
    "                {\"role\": \"user\", \"content\": relevance_check_prompt}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        relevance_info = relevance_response.choices[0].message.content.strip()\n",
    "        print(\"âœ… ê´€ë ¨ì„± í‰ê°€ ì™„ë£Œ\")\n",
    "        \n",
    "        # ì ìˆ˜ ì¶”ì¶œ\n",
    "        relevance_score = \"ì •ë³´ì—†ìŒ\"\n",
    "        for line in relevance_info.split('\\n'):\n",
    "            if 'ì ìˆ˜:' in line:\n",
    "                relevance_score = line.replace('ì ìˆ˜:', '').strip()\n",
    "                break\n",
    "        \n",
    "        print(f\"ğŸ“Š ê´€ë ¨ì„± ì ìˆ˜: {relevance_score}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ê´€ë ¨ì„± í‰ê°€ ì˜¤ë¥˜: {e}\")\n",
    "        relevance_info = \"í‰ê°€ ì‹¤íŒ¨\"\n",
    "        relevance_score = \"ì˜¤ë¥˜\"\n",
    "    \n",
    "    # 2ë‹¨ê³„: ì‹œë†‰ì‹œìŠ¤ ìƒì„±\n",
    "    print(\"\\nğŸ“ 2ë‹¨ê³„: ì‹œë†‰ì‹œìŠ¤ ìƒì„± ì¤‘...\")\n",
    "    synopsis_prompt = f\"\"\"\n",
    "ì§ˆë¬¸: {query}\n",
    "\n",
    "ê´€ë ¨ì„± í‰ê°€: {relevance_info}\n",
    "\n",
    "ì°¸ê³  ìë£Œ:\n",
    "{context}\n",
    "\n",
    "ìœ„ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì›¹íˆ°/ë“œë¼ë§ˆ ìŠ¤íƒ€ì¼ì˜ í¥ë¯¸ì§„ì§„í•œ ì‹œë†‰ì‹œìŠ¤ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”!\n",
    "\n",
    "ìŠ¤íƒ€ì¼ ìš”êµ¬ì‚¬í•­:\n",
    "1. 400-500ì ë¶„ëŸ‰\n",
    "2. ì›¹íˆ°ì´ë‚˜ ì‚¬ê·¹ ë“œë¼ë§ˆì²˜ëŸ¼ í¥ë¯¸ì§„ì§„í•˜ê³  ì—­ë™ì ìœ¼ë¡œ ì‘ì„±\n",
    "3. \"~ë‹¤!\", \"~ì—ˆë‹¤!\", \"í•˜ì§€ë§Œ!\", \"ê·¸ëŸ°ë°!\" ê°™ì€ ê°íƒ„ë¶€í˜¸ ì ê·¹ í™œìš©\n",
    "4. ì¸ë¬¼ë“¤ì˜ ê°ì •ê³¼ ê°ˆë“±ì„ ìƒìƒí•˜ê²Œ ë¬˜ì‚¬\n",
    "5. ê¸´ì¥ê°ê³¼ ë°˜ì „ì´ ìˆëŠ” ìŠ¤í† ë¦¬í…”ë§\n",
    "6. ë…ìê°€ \"ë‹¤ìŒì´ ê¶ê¸ˆí•´!\"ë¼ê³  ëŠë‚„ ìˆ˜ ìˆê²Œ ì‘ì„±\n",
    "7. êµ¬ì²´ì ì¸ ì¸ë¬¼ëª…ê³¼ ì‚¬ê±´ì„ í¬í•¨í•˜ë˜ ë“œë¼ë§ˆí‹±í•˜ê²Œ ê°ìƒ‰\n",
    "\n",
    "ì˜ˆì‹œ í†¤:\n",
    "\"ì™•ì¢Œë¥¼ ë‘˜ëŸ¬ì‹¼ ì¹˜ì—´í•œ ê¶Œë ¥íˆ¬ìŸì´ ì‹œì‘ëœë‹¤! ì˜ì¡°ëŠ” ì™¸ì³¤ë‹¤ - 'ì´ ë‚˜ë¼ì— ë¶•ë‹¹ì€ í•„ìš” ì—†ë‹¤!' í•˜ì§€ë§Œ ì‹ í•˜ë“¤ì˜ ì†ë§ˆìŒì€ ë‹¬ëë‹¤...\"\n",
    "\n",
    "ì‹œë†‰ì‹œìŠ¤:\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        synopsis_response = openai_client.chat.completions.create(\n",
    "            model=AZURE_OPENAI_DEPLOYMENT,\n",
    "            max_tokens=1000,\n",
    "            temperature=1.0,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\", \n",
    "                    \"content\": \"ì—­ì‚¬ì™€ ì´ì•¼ê¸°ë¥¼ ìƒìƒí•˜ê³  ëª°ì…ê° ìˆê²Œ ì „ë‹¬í•˜ëŠ” í•œêµ­ì‚¬ êµìœ¡ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": synopsis_prompt\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        synopsis = synopsis_response.choices[0].message.content.strip()\n",
    "        print(\"âœ… ì‹œë†‰ì‹œìŠ¤ ìƒì„± ì™„ë£Œ\")\n",
    "        print(f\"ğŸ“Š ìƒì„±ëœ ì‹œë†‰ì‹œìŠ¤ ê¸¸ì´: {len(synopsis)}ì\")\n",
    "        \n",
    "        return synopsis, relevance_info, relevance_score\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ì‹œë†‰ì‹œìŠ¤ ìƒì„± ì˜¤ë¥˜: {e}\")\n",
    "        return \"ì‹œë†‰ì‹œìŠ¤ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\", relevance_info, relevance_score\n",
    "\n",
    "def final_evaluation(synopsis, query):\n",
    "    \"\"\"ìµœì¢… í‰ê°€\"\"\"\n",
    "    print_subsection(\"í’ˆì§ˆ í‰ê°€\")\n",
    "    print(\"ğŸ” AI í’ˆì§ˆ í‰ê°€ ì§„í–‰ ì¤‘...\")\n",
    "    \n",
    "    eval_prompt = f\"\"\"\n",
    "ì§ˆë¬¸: {query}\n",
    "ì‘ì„±ëœ ì‹œë†‰ì‹œìŠ¤: {synopsis}\n",
    "\n",
    "ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”:\n",
    "\n",
    "1. ì§ˆë¬¸ ì í•©ì„± (1-5ì ): ì§ˆë¬¸ì˜ í•µì‹¬ì„ ì–¼ë§ˆë‚˜ ì˜ ë‹¤ë¤˜ëŠ”ê°€?\n",
    "2. ë‚´ìš© êµ¬ì²´ì„± (1-5ì ): êµ¬ì²´ì ì¸ ì‚¬ì‹¤ê³¼ ì˜ˆì‹œê°€ í¬í•¨ë˜ì—ˆëŠ”ê°€?\n",
    "3. êµìœ¡ì  ê°€ì¹˜ (1-5ì ): í•™ìŠµì— ë„ì›€ì´ ë˜ëŠ”ê°€?\n",
    "4. ì™„ì„±ë„ (1-5ì ): ì „ì²´ì ìœ¼ë¡œ ì™„ì„±ë„ê°€ ë†’ì€ê°€?\n",
    "\n",
    "í‰ê°€ í˜•ì‹:\n",
    "1. ì§ˆë¬¸ ì í•©ì„±: X/5ì  - ì„¤ëª…\n",
    "2. ë‚´ìš© êµ¬ì²´ì„±: X/5ì  - ì„¤ëª…  \n",
    "3. êµìœ¡ì  ê°€ì¹˜: X/5ì  - ì„¤ëª…\n",
    "4. ì™„ì„±ë„: X/5ì  - ì„¤ëª…\n",
    "\n",
    "ì´ì : XX/20ì \n",
    "\n",
    "ì¥ì :\n",
    "- ì¥ì 1\n",
    "- ì¥ì 2\n",
    "\n",
    "ê°œì„ ì :\n",
    "- ê°œì„ ì 1\n",
    "- ê°œì„ ì 2\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        eval_response = openai_client.chat.completions.create(\n",
    "            model=AZURE_OPENAI_DEPLOYMENT,\n",
    "            max_tokens=600,\n",
    "            temperature=0.5,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"êµìœ¡ ì½˜í…ì¸  í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\"},\n",
    "                {\"role\": \"user\", \"content\": eval_prompt}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        evaluation = eval_response.choices[0].message.content.strip()\n",
    "        print(\"âœ… í’ˆì§ˆ í‰ê°€ ì™„ë£Œ\")\n",
    "        \n",
    "        # ì´ì  ì¶”ì¶œ\n",
    "        total_score = \"ì •ë³´ì—†ìŒ\"\n",
    "        for line in evaluation.split('\\n'):\n",
    "            if 'ì´ì :' in line:\n",
    "                total_score = line.replace('ì´ì :', '').strip()\n",
    "                break\n",
    "        \n",
    "        print(f\"ğŸ“Š ìµœì¢… ì ìˆ˜: {total_score}\")\n",
    "        \n",
    "        return evaluation, total_score\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ í’ˆì§ˆ í‰ê°€ ì˜¤ë¥˜: {e}\")\n",
    "        return \"í‰ê°€ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\", \"ì˜¤ë¥˜\"\n",
    "\n",
    "def display_final_results(result):\n",
    "    \"\"\"ìµœì¢… ê²°ê³¼ ì¶œë ¥\"\"\"\n",
    "    print_header(\"ìµœì¢… ë¶„ì„ ê²°ê³¼\", width=100)\n",
    "    \n",
    "    # ê¸°ë³¸ ì •ë³´\n",
    "    print_subsection(\"ğŸ¯ ë¶„ì„ ê°œìš”\")\n",
    "    print(f\"ğŸ“ ë¶„ì„ ì§ˆì˜: {result['query']}\")\n",
    "    print(f\"â° ë¶„ì„ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"ğŸ“Š ì¶”ì¶œëœ í‚¤ì›Œë“œ: {len(result.get('keywords', []))}ê°œ\")\n",
    "    print(f\"ğŸ“Š ì‚¬ìš©ëœ ë¬¸ì„œ: {len(result['relevant_docs'])}ê°œ\")\n",
    "    print(f\"ğŸ“Š ê´€ë ¨ì„± ì ìˆ˜: {result.get('relevance_score', 'ì •ë³´ì—†ìŒ')}\")\n",
    "    print(f\"ğŸ“Š ìµœì¢… í’ˆì§ˆ ì ìˆ˜: {result.get('total_score', 'ì •ë³´ì—†ìŒ')}\")\n",
    "    \n",
    "    # í‚¤ì›Œë“œ ì •ë³´\n",
    "    if result.get('keyword_info'):\n",
    "        print_subsection(\"ğŸ”‘ ì¶”ì¶œëœ í‚¤ì›Œë“œ ì •ë³´\")\n",
    "        kw_info = result['keyword_info']\n",
    "        if kw_info.get('core'):\n",
    "            print(f\"í•µì‹¬í‚¤ì›Œë“œ: {', '.join(kw_info['core'][:5])}\")\n",
    "        if kw_info.get('extended'):\n",
    "            print(f\"í™•ì¥í‚¤ì›Œë“œ: {', '.join(kw_info['extended'][:5])}\")\n",
    "        if kw_info.get('persons'):\n",
    "            print(f\"ì¸ë¬¼í‚¤ì›Œë“œ: {', '.join(kw_info['persons'][:3])}\")\n",
    "    \n",
    "    # ê²€ìƒ‰ëœ ë¬¸ì„œ ìš”ì•½\n",
    "    print_subsection(\"ğŸ“š ì£¼ìš” ë°œê²¬ ë¬¸ì„œ\")\n",
    "    for i, doc in enumerate(result['relevant_docs'][:3], 1):\n",
    "        clean_content = ' '.join(doc['preview'].replace('\\n', ' ').split())\n",
    "        print(f\"{i}. í‚¤ì›Œë“œ: [{doc['keyword']}] | ê²€ìƒ‰ì ìˆ˜: {doc['score']:.2f}\")\n",
    "        print(f\"   ğŸ“„ ë‚´ìš©: {clean_content}\")\n",
    "        print()\n",
    "    \n",
    "    # ìƒì„±ëœ ì‹œë†‰ì‹œìŠ¤\n",
    "    print_subsection(\"ğŸ“– ìƒì„±ëœ ì‹œë†‰ì‹œìŠ¤\")\n",
    "    print(\"â•”\" + \"â•\" * 78 + \"â•—\")\n",
    "    synopsis_lines = result['synopsis'].split('\\n')\n",
    "    for line in synopsis_lines:\n",
    "        if len(line) > 76:\n",
    "            words = line.split(' ')\n",
    "            current_line = \"\"\n",
    "            for word in words:\n",
    "                if len(current_line + word) <= 76:\n",
    "                    current_line += word + \" \"\n",
    "                else:\n",
    "                    print(f\"â•‘ {current_line.ljust(76)} â•‘\")\n",
    "                    current_line = word + \" \"\n",
    "            if current_line.strip():\n",
    "                print(f\"â•‘ {current_line.strip().ljust(76)} â•‘\")\n",
    "        else:\n",
    "            print(f\"â•‘ {line.ljust(76)} â•‘\")\n",
    "    print(\"â•š\" + \"â•\" * 78 + \"â•\")\n",
    "    \n",
    "    # í’ˆì§ˆ í‰ê°€\n",
    "    print_subsection(\"ğŸ“Š í’ˆì§ˆ í‰ê°€ ê²°ê³¼\")\n",
    "    eval_lines = result['evaluation'].split('\\n')\n",
    "    for line in eval_lines:\n",
    "        if line.strip():\n",
    "            if any(keyword in line for keyword in ['ì§ˆë¬¸ ì í•©ì„±', 'ë‚´ìš© êµ¬ì²´ì„±', 'êµìœ¡ì  ê°€ì¹˜', 'ì™„ì„±ë„']):\n",
    "                print(f\"  ğŸ“ˆ {line}\")\n",
    "            elif 'ì´ì :' in line:\n",
    "                print(f\"  ğŸ¯ {line}\")\n",
    "            elif line.startswith('ì¥ì :') or line.startswith('ê°œì„ ì :'):\n",
    "                print(f\"  ğŸ“Œ {line}\")\n",
    "            elif line.startswith('- '):\n",
    "                print(f\"    {line}\")\n",
    "            else:\n",
    "                print(f\"  {line}\")\n",
    "\n",
    "def ultimate_rag_solution(query, top_k=10, max_final_docs=10):\n",
    "    \"\"\"\n",
    "    ì¿¼ë¦¬ ê¸°ë°˜ ìë™ RAG ì‹œìŠ¤í…œ - AIê°€ í‚¤ì›Œë“œë¥¼ ìë™ ì¶”ì¶œ\n",
    "    \n",
    "    Args:\n",
    "        query (str): ì‚¬ìš©ì ì…ë ¥ ì¿¼ë¦¬ (ì–´ë–¤ ì£¼ì œë“  ê°€ëŠ¥)\n",
    "        top_k (int): í‚¤ì›Œë“œë‹¹ ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜\n",
    "        max_final_docs (int): ìµœì¢… ì„ ë³„í•  ë¬¸ì„œ ìˆ˜\n",
    "    \n",
    "    Returns:\n",
    "        dict: ë¶„ì„ ê²°ê³¼\n",
    "    \"\"\"\n",
    "    print_header(f\"AI ìë™ í‚¤ì›Œë“œ ì¶”ì¶œ RAG ì‹œìŠ¤í…œ\", width=100)\n",
    "    print(f\"ğŸš€ ì‹œìŠ¤í…œ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"ğŸ“ ë¶„ì„ ì£¼ì œ: {query}\")\n",
    "    print(f\"ğŸ¯ ê²€ìƒ‰ ì„¤ì •: í‚¤ì›Œë“œë‹¹ {top_k}ê°œ, ìµœì¢… {max_final_docs}ê°œ ë¬¸ì„œ\")\n",
    "    \n",
    "    # 1ë‹¨ê³„: AI í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë¬¸ì„œ ê²€ìƒ‰\n",
    "    print_section(\"1ï¸âƒ£ AI í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë¬¸ì„œ ê²€ìƒ‰\")\n",
    "    relevant_docs, keyword_info = find_documents_from_query(query, top_k, max_final_docs)\n",
    "    \n",
    "    if not relevant_docs:\n",
    "        print(\"âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return None\n",
    "    \n",
    "    # ì¶”ì¶œëœ í‚¤ì›Œë“œ ì •ë³´\n",
    "    keywords = []\n",
    "    if keyword_info:\n",
    "        keywords.extend(keyword_info.get('core', []))\n",
    "        keywords.extend(keyword_info.get('extended', []))\n",
    "        keywords.extend(keyword_info.get('persons', []))\n",
    "    \n",
    "    # 2ë‹¨ê³„: ì»¨í…ìŠ¤íŠ¸ ìƒì„±\n",
    "    print_section(\"2ï¸âƒ£ ì»¨í…ìŠ¤íŠ¸ ìƒì„± ë‹¨ê³„\")\n",
    "    context = create_context_from_best_docs(relevant_docs)\n",
    "    \n",
    "    # 3ë‹¨ê³„: ì‹œë†‰ì‹œìŠ¤ ìƒì„±\n",
    "    print_section(\"3ï¸âƒ£ ì‹œë†‰ì‹œìŠ¤ ìƒì„± ë‹¨ê³„\")\n",
    "    synopsis, relevance_info, relevance_score = smart_synopsis_generation(query, context)\n",
    "    \n",
    "    # 4ë‹¨ê³„: í’ˆì§ˆ í‰ê°€\n",
    "    print_section(\"4ï¸âƒ£ í’ˆì§ˆ í‰ê°€ ë‹¨ê³„\")\n",
    "    evaluation, total_score = final_evaluation(synopsis, query)\n",
    "    \n",
    "    # ê²°ê³¼ ì •ë¦¬\n",
    "    result = {\n",
    "        'query': query,\n",
    "        'keywords': keywords,\n",
    "        'keyword_info': keyword_info,\n",
    "        'search_settings': {\n",
    "            'top_k': top_k,\n",
    "            'max_final_docs': max_final_docs\n",
    "        },\n",
    "        'relevant_docs': relevant_docs,\n",
    "        'context': context,\n",
    "        'synopsis': synopsis,\n",
    "        'relevance_info': relevance_info,\n",
    "        'relevance_score': relevance_score,\n",
    "        'evaluation': evaluation,\n",
    "        'total_score': total_score,\n",
    "        'timestamp': datetime.now()\n",
    "    }\n",
    "    \n",
    "    # 5ë‹¨ê³„: ìµœì¢… ê²°ê³¼ ì¶œë ¥\n",
    "    print_section(\"5ï¸âƒ£ ìµœì¢… ê²°ê³¼\")\n",
    "    display_final_results(result)\n",
    "    \n",
    "    print_header(\"ë¶„ì„ ì™„ë£Œ\", width=100)\n",
    "    print(f\"ğŸ ë¶„ì„ ì¢…ë£Œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"ì¡°ì„ ì‹œëŒ€ ì™•ê³¼ ì‹ í•˜ ê°ˆë“±\"\n",
    "    result = ultimate_rag_solution(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
